{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, functools, itertools, collections, time, random, pickle, warnings, json, subprocess, shutil\n",
    "pkg_path = '/home/jupyter/code'\n",
    "if pkg_path not in sys.path:\n",
    "    sys.path.append(pkg_path)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utility import get_label_image, get_cor, get_cor_map, get_topk_indices, get_cor_map_4d, get_local_mean\n",
    "from utility import get_prime_factors\n",
    "from visualization import imshow, plot_image_label_overlay, make_video_ffmpeg, get_good_colors, plot_colortable\n",
    "from models import UNet\n",
    "from denoise import get_denoised_mat, model_denoise, SeparateNet\n",
    "from segmentation import get_traces\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = 'noise2self'\n",
    "use_existing_config = True\n",
    "use_pretrained_model = True\n",
    "use_importance_sampling = False\n",
    "save_intermediate_results = False\n",
    "verbose = True\n",
    "ndim = 2\n",
    "out_channels =  [64, 64, 128]\n",
    "frame_depth = 4\n",
    "kernel_size = 3\n",
    "last_out_channels = [100]\n",
    "separate_net = False\n",
    "batch_size = 5\n",
    "batch_size_eval = 5\n",
    "nrow = 180\n",
    "ncol = 512\n",
    "num_global_features = 4\n",
    "window_size_row = None # 96\n",
    "window_size_col = None # 96\n",
    "\n",
    "num_epochs = 26\n",
    "num_iters = 500\n",
    "mask_prob = 0.05\n",
    "loss_threshold = 1e-3\n",
    "loss_reg_fn = nn.MSELoss()\n",
    "optimizer_fn_args = {'lr': 1e-3, 'weight_decay': 1e-2}  \n",
    "lr_scheduler = {'lr_fn': lambda epoch, num_epochs, lr0, log_fold_change: lr0 * np.exp(-log_fold_change/max(num_epochs-1, 1)*epoch), \n",
    "                'lr_fn_args': {'lr0': optimizer_fn_args['lr'], 'num_epochs': num_epochs, 'log_fold_change': np.log(10)}}\n",
    "optimizer_fn = torch.optim.AdamW\n",
    "movie_start_idx = 100\n",
    "movie_end_idx = 700\n",
    "\n",
    "if separate_net:\n",
    "    num_features = 32\n",
    "    temporal_out_channels=[32, 32, 32, 1]\n",
    "    num_conv_temporal = len(temporal_out_channels)\n",
    "    assert temporal_out_channels[-1] == 1\n",
    "    assert (2*frame_depth) % num_conv_temporal == 0\n",
    "\n",
    "if use_existing_config and os.path.exists(f'{model_folder}/config.json'):\n",
    "    with open(f'{model_folder}/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "        print('Use existing config.json:\\n', json.dumps(config, indent=2))\n",
    "        ndim = config['default_config']['model_config']['ndim']\n",
    "        out_channels = config['default_config']['model_config']['out_channels']\n",
    "        frame_depth = config['default_config']['model_config']['frame_depth']\n",
    "        kernel_size = config['default_config']['model_config']['kernel_size']\n",
    "        last_out_channels = config['default_config']['model_config']['last_out_channels']\n",
    "        separate_net = config['default_config']['model_config']['separate_net']\n",
    "        if separate_net:\n",
    "            num_features = config['default_config']['model_config']['num_features']\n",
    "            temporal_out_channels = config['default_config']['model_config']['temporal_out_channels']\n",
    "            num_conv_temporal = len(temporal_out_channels)\n",
    "else:\n",
    "    config = {'default_config': {'model_config': \n",
    "                             {'ndim': ndim, 'out_channels': out_channels, 'frame_depth': frame_depth, 'kernel_size': kernel_size,\n",
    "                              'last_out_channels': last_out_channels, 'separate_net': separate_net}}}\n",
    "    if config['default_config']['model_config']['separate_net']:\n",
    "        config['default_config']['model_config'].update({'num_features': num_features, 'temporal_out_channels': temporal_out_channels})\n",
    "        \n",
    "print_every = max(num_iters // 2, 1)\n",
    "assert kernel_size%2==1\n",
    "encoder_depth = len(out_channels)\n",
    "padding = (kernel_size-1)//2\n",
    "pool_kernel_size_row = get_prime_factors(nrow)[:encoder_depth]\n",
    "pool_kernel_size_col = get_prime_factors(ncol)[:encoder_depth]\n",
    "if window_size_row is not None and window_size_col is not None:\n",
    "    assert window_size_row % np.product(pool_kernel_size_row) == 0\n",
    "    assert window_size_col % np.product(pool_kernel_size_col) == 0\n",
    "\n",
    "if ndim == 2:\n",
    "    if separate_net:\n",
    "        k = 2*frame_depth//num_conv_temporal + 1\n",
    "        model = SeparateNet(in_channels=1, num_features=num_features, spatial_out_channels=out_channels,\n",
    "                 num_conv=2, kernel_size=kernel_size, padding=padding, \n",
    "                 pool_kernel_size=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in range(encoder_depth)],  \n",
    "                 transpose_kernel_size=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))],\n",
    "                 transpose_stride=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                 last_out_channels=last_out_channels, \n",
    "                 use_adaptive_pooling=False, padding_mode='replicate', \n",
    "                 temporal_out_channels=temporal_out_channels, temporal_kernel_size=(k, 1, 1),\n",
    "                 normalization='layer_norm', activation=nn.LeakyReLU(negative_slope=0.01, inplace=True)).to(device)\n",
    "    else:\n",
    "        in_channels = frame_depth*2 + 1 + num_global_features\n",
    "        model = UNet(in_channels=in_channels, num_classes=1, out_channels=out_channels, num_conv=2, n_dim=2, \n",
    "                     kernel_size=kernel_size, \n",
    "                     padding=padding, \n",
    "                     pool_kernel_size=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in range(encoder_depth)], \n",
    "                     transpose_kernel_size=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                     transpose_stride=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                     last_out_channels=last_out_channels, \n",
    "                     use_adaptive_pooling=False, same_shape=True, padding_mode='replicate', normalization='layer_norm',\n",
    "                     activation=nn.LeakyReLU(negative_slope=0.01, inplace=True)).to(device)\n",
    "elif ndim == 3:\n",
    "    assert frame_depth % encoder_depth == 0\n",
    "    k = frame_depth//encoder_depth + 1\n",
    "    model = UNet(in_channels=1, num_classes=1, out_channels=out_channels, num_conv=2, n_dim=3, \n",
    "             kernel_size=[kernel_size] + [(k, kernel_size, kernel_size)]*encoder_depth + [(1, kernel_size, kernel_size)]*encoder_depth, \n",
    "                 padding=[padding] + [(0, padding, padding)]*encoder_depth*2,\n",
    "                 pool_kernel_size=[(1, pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in range(encoder_depth)], \n",
    "                 transpose_kernel_size=[(1, pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                 transpose_stride=[(1, pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                 last_out_channels=last_out_channels,\n",
    "                 use_adaptive_pooling=True, same_shape=False, padding_mode='zeros', normalization='layer_norm', \n",
    "                 activation=nn.LeakyReLU(negative_slope=0.01, inplace=True)).to(device)\n",
    "cnt = 0\n",
    "for n, p in model.named_parameters():\n",
    "    cnt += p.numel()\n",
    "print(f'Total number of model parameters = {cnt}')\n",
    "config['default_config']['model_config'].update({'num_total_params': cnt})\n",
    "\n",
    "if not os.path.exists(model_folder):\n",
    "    print(f'Creating folder {model_folder}')\n",
    "    os.makedirs(model_folder)\n",
    "\n",
    "loss_history = []\n",
    "if use_pretrained_model and os.path.exists(f'{model_folder}/loss__denoise.npy'):\n",
    "    loss_history = np.load(f'{model_folder}/loss__denoise.npy').tolist()\n",
    "    print(f'loading model state {model_folder}/model_step{len(loss_history)}.pt')\n",
    "    model.load_state_dict(torch.load(f'{model_folder}/model_step{len(loss_history)}.pt'))\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.plot(loss_history, 'ro-', markersize=3)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel(f'iteration')\n",
    "    plt.title(f'loss_history[{len(loss_history)}] = ({loss_history[-1]:.3e})')\n",
    "    plt.show()\n",
    "    \n",
    "if use_importance_sampling:\n",
    "    frame_mean = mat.mean((1, 2))\n",
    "    frame_weight = frame_mean - frame_mean.min()\n",
    "    frame_weight = frame_weight + frame_weight.median()\n",
    "    frame_weight /= frame_weight.sum()\n",
    "    frame_weight = frame_weight.cpu().numpy()\n",
    "else:\n",
    "    frame_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_data_folder = 'gs://broad-opp-voltage/sami_2015/processed'\n",
    "command = ['gsutil', 'ls', bucket_data_folder]\n",
    "response = subprocess.run(command, capture_output=True)\n",
    "assert response.returncode == 0\n",
    "data_folders = [s.split('/')[-2] for s in response.stdout.decode().split()]\n",
    "data_folders = sorted([data_folder for data_folder in data_folders if re.search('FOV', data_folder) and re.search('at', data_folder)])\n",
    "data_folders = data_folders[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data_folder in enumerate(data_folders):\n",
    "    if not os.path.exists(data_folder):\n",
    "        print(f'Create {data_folder}')\n",
    "        os.makedirs(data_folder)\n",
    "    if not os.path.exists(f'{data_folder}/mat.npy'):\n",
    "        command = ['gsutil', '-m', 'cp', f'{bucket_data_folder}/{data_folder}/mat.npy', f'{data_folder}/mat.npy']\n",
    "        response = subprocess.run(command, capture_output=True)\n",
    "        assert response.returncode == 0\n",
    "    mat = torch.from_numpy(np.load(f'{data_folder}/mat.npy')).float().to(device)\n",
    "    assert mat.shape[1] == nrow and mat.shape[2] == ncol\n",
    "    if not os.path.exists(f'{data_folder}/features.npy'):\n",
    "        command = ['gsutil', '-m', 'cp', f'{bucket_data_folder}/{data_folder}/features.npy', f'{data_folder}/features.npy']\n",
    "        response = subprocess.run(command, capture_output=True)\n",
    "        assert response.returncode == 0\n",
    "    features = torch.from_numpy(np.load(f'{data_folder}/features.npy')).float().to(device)\n",
    "    assert num_global_features == features.shape[0]\n",
    "\n",
    "    save_folder = f'{data_folder}/{model_folder}'\n",
    "    if not os.path.exists(save_folder):\n",
    "        print(f'Create folder {save_folder}')\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    train_interval = [0, mat.shape[0]]\n",
    "    train_index = range(*train_interval)\n",
    "    start_time = time.time()\n",
    "    denoised_mat, model = get_denoised_mat(mat[train_index], model=model, ndim=ndim, out_channels=out_channels, \n",
    "                                           num_epochs=num_epochs, num_iters=num_iters, \n",
    "                                           print_every=print_every, batch_size=batch_size, mask_prob=mask_prob, \n",
    "                                           frame_depth=frame_depth, frame_weight=frame_weight, \n",
    "                                           movie_start_idx=movie_start_idx, movie_end_idx=movie_end_idx, \n",
    "                                           save_folder=save_folder, \n",
    "                                           save_intermediate_results=save_intermediate_results, normalize=True, \n",
    "                                           last_out_channels=last_out_channels, \n",
    "                                           loss_reg_fn=loss_reg_fn, loss_history=loss_history, \n",
    "                                           loss_threshold=0 if i==0 else loss_threshold, \n",
    "                                           optimizer_fn=optimizer_fn, optimizer_fn_args=optimizer_fn_args, lr_scheduler=lr_scheduler,\n",
    "                                           batch_size_eval=batch_size_eval, features=features,\n",
    "                                           window_size_row=window_size_row, window_size_col=window_size_col, weight=None,\n",
    "                                           verbose=verbose, return_model=True, device=device)\n",
    "    if mat[train_index].shape[0] < mat.shape[0]:\n",
    "        denoised_mat = model_denoise(mat, model, ndim=ndim, frame_depth=frame_depth, features=features, batch_size=batch_size_eval, \n",
    "                                     normalize=True)\n",
    "        np.save(f'{save_folder}/denoised_movie_frame0to{mat.shape[0]}_step{len(loss_history)}.npy', denoised_mat.cpu().numpy())\n",
    "    end_time = time.time()\n",
    "\n",
    "    num_episodes = len([k for k in config.keys() if re.search('^episode', k)])\n",
    "    config[f'episode{num_episodes}'] = {'train_settings': \n",
    "                                       {'num_epochs': num_epochs, 'num_iters': num_iters, 'batch_size': batch_size, \n",
    "                                        'batch_size_eval': batch_size_eval, 'mask_prob': mask_prob,\n",
    "                                        'use_importance_sampling': use_importance_sampling, 'time_spent': end_time-start_time,\n",
    "                                        'optimizer_fn_args': optimizer_fn_args, 'train_interval': train_interval\n",
    "                                       }}\n",
    "    if lr_scheduler is not None:\n",
    "        config[f'episode{num_episodes}']['train_settings']['lr_scheduler'] = (\n",
    "            [lr_scheduler['lr_fn'](epoch, **lr_scheduler['lr_fn_args']) for epoch in range(num_epochs)])\n",
    "\n",
    "    with open(f'{save_folder}/config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    command = ['gsutil', '-m', 'cp', f'{save_folder}/*', f'{bucket_data_folder}/{save_folder}/']\n",
    "    response = subprocess.run(command, capture_output=True)\n",
    "    assert response.returncode == 0\n",
    "    del mat, features, denoised_mat\n",
    "    torch.cuda.empty_cache()\n",
    "    shutil.rmtree(data_folder)\n",
    "\n",
    "with open(f'{model_folder}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "torch.save(model.state_dict(), f'{model_folder}/model_step{len(loss_history)}.pt')\n",
    "np.save(f'{model_folder}/loss__denoise.npy', loss_history)\n",
    "command = ['gsutil', '-m', 'cp', '-r', f'{model_folder}', f'{bucket_data_folder}']\n",
    "response = subprocess.run(command, capture_output=True)\n",
    "assert response.returncode == 0\n",
    "shutil.rmtree(model_folder)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
