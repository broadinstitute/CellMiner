{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, functools, itertools, collections, time, random, pickle, warnings, json\n",
    "pkg_path = '/home/jupyter/code'\n",
    "if pkg_path not in sys.path:\n",
    "    sys.path.append(pkg_path)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utility import get_label_image, get_cor, get_cor_map, get_topk_indices, get_cor_map_4d, get_local_mean\n",
    "from utility import get_prime_factors\n",
    "from visualization import imshow, plot_image_label_overlay, make_video_ffmpeg, get_good_colors, plot_colortable\n",
    "from models import UNet\n",
    "from denoise import get_denoised_mat, model_denoise, SeparateNet\n",
    "from segmentation import get_traces\n",
    "\n",
    "use_gpu = True\n",
    "if use_gpu and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = torch.from_numpy(np.load('../data/mat.npy')).float().to(device)\n",
    "nframe, nrow, ncol = mat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise2Self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = '2d-noise2self'\n",
    "use_existing_config = True\n",
    "use_pretrained_model = True\n",
    "use_importance_sampling = False\n",
    "ndim = 2\n",
    "out_channels =  [64, 64, 128] #[64, 64, 64] #[32, 32, 32]\n",
    "frame_depth = 4\n",
    "kernel_size = 3\n",
    "last_out_channels = [100]\n",
    "separate_net = False\n",
    "batch_size = 10\n",
    "batch_size_eval = 20\n",
    "\n",
    "num_epochs = 4\n",
    "num_iters = 6000\n",
    "mask_prob = 0.05\n",
    "loss_threshold = 0\n",
    "loss_reg_fn = nn.MSELoss()\n",
    "optimizer_fn_args = {'lr': 1e-3, 'weight_decay': 1e-2}  \n",
    "lr_scheduler = {'lr_fn': lambda epoch, num_epochs, lr0, log_fold_change: lr0 * np.exp(-log_fold_change/max(num_epochs-1, 1)*epoch), \n",
    "                'lr_fn_args': {'lr0': optimizer_fn_args['lr'], 'num_epochs': num_epochs, 'log_fold_change': np.log(10)}}\n",
    "optimizer_fn = torch.optim.AdamW\n",
    "movie_start_idx = 250\n",
    "movie_end_idx = 750\n",
    "\n",
    "if separate_net:\n",
    "    num_features = 32\n",
    "    temporal_out_channels=[32, 32, 32, 1]\n",
    "    num_conv_temporal = len(temporal_out_channels)\n",
    "    assert temporal_out_channels[-1] == 1\n",
    "    assert (2*frame_depth) % num_conv_temporal == 0\n",
    "\n",
    "if use_existing_config and os.path.exists(f'{save_folder}/config.json'):\n",
    "    with open(f'{save_folder}/config.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "        print('Use existing config.json:\\n', json.dumps(config, indent=2))\n",
    "        ndim = config['default_config']['model_config']['ndim']\n",
    "        out_channels = config['default_config']['model_config']['out_channels']\n",
    "        frame_depth = config['default_config']['model_config']['frame_depth']\n",
    "        kernel_size = config['default_config']['model_config']['kernel_size']\n",
    "        last_out_channels = config['default_config']['model_config']['last_out_channels']\n",
    "        separate_net = config['default_config']['model_config']['separate_net']\n",
    "        if separate_net:\n",
    "            num_features = config['default_config']['model_config']['num_features']\n",
    "            temporal_out_channels = config['default_config']['model_config']['temporal_out_channels']\n",
    "            num_conv_temporal = len(temporal_out_channels)\n",
    "else:\n",
    "    config = {'default_config': {'model_config': \n",
    "                             {'ndim': ndim, 'out_channels': out_channels, 'frame_depth': frame_depth, 'kernel_size': kernel_size,\n",
    "                              'last_out_channels': last_out_channels, 'separate_net': separate_net}}}\n",
    "    if config['default_config']['model_config']['separate_net']:\n",
    "        config['default_config']['model_config'].update({'num_features': num_features, 'temporal_out_channels': temporal_out_channels})\n",
    "        \n",
    "print_every = max(num_iters // 2, 1)\n",
    "assert kernel_size%2==1\n",
    "encoder_depth = len(out_channels)\n",
    "padding = (kernel_size-1)//2\n",
    "pool_kernel_size_row = get_prime_factors(nrow)[:encoder_depth]\n",
    "pool_kernel_size_col = get_prime_factors(ncol)[:encoder_depth]\n",
    "\n",
    "if ndim == 2:\n",
    "    if separate_net:\n",
    "        k = 2*frame_depth//num_conv_temporal + 1\n",
    "        model = SeparateNet(in_channels=1, num_features=num_features, spatial_out_channels=out_channels,\n",
    "                 num_conv=2, kernel_size=kernel_size, padding=padding, \n",
    "                 pool_kernel_size=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in range(encoder_depth)],  \n",
    "                 transpose_kernel_size=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))],\n",
    "                 transpose_stride=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                 last_out_channels=last_out_channels, \n",
    "                 use_adaptive_pooling=False, padding_mode='replicate', \n",
    "                 temporal_out_channels=temporal_out_channels, temporal_kernel_size=(k, 1, 1),\n",
    "                 normalization='layer_norm', activation=nn.LeakyReLU(negative_slope=0.01, inplace=True)).to(device)\n",
    "    else:\n",
    "        in_channels = frame_depth*2 + 1\n",
    "        model = UNet(in_channels=in_channels, num_classes=1, out_channels=out_channels, num_conv=2, n_dim=2, \n",
    "                     kernel_size=kernel_size, \n",
    "                     padding=padding, \n",
    "                     pool_kernel_size=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in range(encoder_depth)], \n",
    "                     transpose_kernel_size=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                     transpose_stride=[(pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                     last_out_channels=last_out_channels, \n",
    "                     use_adaptive_pooling=False, same_shape=True, padding_mode='replicate', normalization='layer_norm',\n",
    "                     activation=nn.LeakyReLU(negative_slope=0.01, inplace=True)).to(device)\n",
    "elif ndim == 3:\n",
    "    assert frame_depth % encoder_depth == 0\n",
    "    k = frame_depth//encoder_depth + 1\n",
    "    model = UNet(in_channels=1, num_classes=1, out_channels=out_channels, num_conv=2, n_dim=3, \n",
    "             kernel_size=[kernel_size] + [(k, kernel_size, kernel_size)]*encoder_depth + [(1, kernel_size, kernel_size)]*encoder_depth, \n",
    "                 padding=[padding] + [(0, padding, padding)]*encoder_depth*2,\n",
    "                 pool_kernel_size=[(1, pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in range(encoder_depth)], \n",
    "                 transpose_kernel_size=[(1, pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                 transpose_stride=[(1, pool_kernel_size_row[i], pool_kernel_size_col[i]) for i in reversed(range(encoder_depth))], \n",
    "                 last_out_channels=last_out_channels,\n",
    "                 use_adaptive_pooling=True, same_shape=False, padding_mode='zeros', normalization='layer_norm', \n",
    "                 activation=nn.LeakyReLU(negative_slope=0.01, inplace=True)).to(device)\n",
    "cnt = 0\n",
    "for n, p in model.named_parameters():\n",
    "    cnt += p.numel()\n",
    "print(f'Total number of model parameters = {cnt}')\n",
    "config['default_config']['model_config'].update({'num_total_params': cnt})\n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    print(f'Creating folder {save_folder}')\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "loss_history = []\n",
    "if use_pretrained_model and os.path.exists(f'{save_folder}/loss__denoise.npy'):\n",
    "    loss_history = np.load(f'{save_folder}/loss__denoise.npy').tolist()\n",
    "    print(f'loading model state {save_folder}/model_step{len(loss_history)}.pt')\n",
    "    model.load_state_dict(torch.load(f'{save_folder}/model_step{len(loss_history)}.pt'))\n",
    "    plt.figure(figsize=(15, 9))\n",
    "    plt.plot(loss_history, 'o-', markersize=3)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel(f'iteration')\n",
    "    plt.title(f'loss_history[{len(loss_history)}] = ({loss_history[-1]:.3e})')\n",
    "    plt.show()\n",
    "    \n",
    "if use_importance_sampling:\n",
    "    frame_mean = mat.mean((1, 2))\n",
    "    frame_weight = frame_mean - frame_mean.min()\n",
    "    frame_weight = frame_weight + frame_weight.median()\n",
    "    frame_weight /= frame_weight.sum()\n",
    "    frame_weight = frame_weight.cpu().numpy()\n",
    "else:\n",
    "    frame_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_interval = [0, mat.shape[0]]\n",
    "train_index = range(*train_interval)\n",
    "start_time = time.time()\n",
    "denoised_mat, model = get_denoised_mat(mat[train_index], model=model, ndim=ndim, out_channels=out_channels, \n",
    "                                       num_epochs=num_epochs, num_iters=num_iters, \n",
    "                                       print_every=print_every, batch_size=batch_size, mask_prob=mask_prob, \n",
    "                                       frame_depth=frame_depth, frame_weight=frame_weight, \n",
    "                                       movie_start_idx=movie_start_idx, movie_end_idx=movie_end_idx, \n",
    "                                       save_folder=save_folder, \n",
    "                                       save_intermediate_results=True, normalize=True, \n",
    "                                       last_out_channels=last_out_channels, \n",
    "                                       loss_reg_fn=loss_reg_fn, loss_history=loss_history, loss_threshold=loss_threshold, \n",
    "                                       optimizer_fn=optimizer_fn, optimizer_fn_args=optimizer_fn_args, lr_scheduler=lr_scheduler,\n",
    "                                       batch_size_eval=batch_size_eval, \n",
    "                                       verbose=True, return_model=True, device=device)\n",
    "end_time = time.time()\n",
    "\n",
    "num_episodes = len([k for k in config.keys() if re.search('^episode', k)])\n",
    "config[f'episode{num_episodes}'] = {'train_settings': \n",
    "                                   {'num_epochs': num_epochs, 'num_iters': num_iters, 'batch_size': batch_size, \n",
    "                                    'batch_size_eval': batch_size_eval, 'mask_prob': mask_prob,\n",
    "                                    'use_importance_sampling': use_importance_sampling, 'time_spent': end_time-start_time,\n",
    "                                    'optimizer_fn_args': optimizer_fn_args, 'train_interval': train_interval\n",
    "                                   }}\n",
    "if lr_scheduler is not None:\n",
    "    config[f'episode{num_episodes}']['train_settings']['lr_scheduler'] = (\n",
    "        [lr_scheduler['lr_fn'](epoch, **lr_scheduler['lr_fn_args']) for epoch in range(num_epochs)])\n",
    "with open(f'{save_folder}/config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mat[train_index].shape[0] < mat.shape[0]:\n",
    "    start_time = time.time()\n",
    "    denoised_mat = model_denoise(mat, model, ndim=ndim, frame_depth=frame_depth, normalize=True, batch_size=batch_size_eval)\n",
    "    end_time = time.time()\n",
    "    np.save(f'{save_folder}/denoised_movie_full_step{len(loss_history)}.npy', denoised_mat.cpu().numpy())\n",
    "    print(end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
